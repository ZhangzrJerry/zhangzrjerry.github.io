# Networked Sensing, Estimation and Control

> something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something, something,
>
> &nbsp; _-- by Ling Shi_{style="float:right"}

<CenteredImg src="/public/posts/elec-5650/cover.png" width=75% />

This is the lecture notes for "ELEC 5650: Networked Sensing, Estimation and Control" in the 2024-25 Spring semester, delivered by Prof. Ling Shi at HKUST. The notes covered fundamental concepts in networked systems, including necessary mathematical tools, estimation theory, Kalman filter, and Least Square Regulator.

<Badges>
<img src="/public/tags/hkust.svg">
<img src="/public/tags/sense.svg">
<img src="/public/tags/control.svg">
</Badges>

## Mathematic Tools

### Probability

$$
E[XY]=E[X]E[Y]+\text{Cov}(X,Y)
$$

### Eigenvalues

$A\in\mathbb R^{n\times n}$, $\lambda$ can be solved by

$$\text{det}(\lambda I-A)=0$$

$$
\prod_{i=1}^n\lambda_i=\text{det}(A),\quad\sum_{i=1}^n\lambda_i=\text{Tr}(A),\quad Av_i=\lambda_iv_i
$$

**Lemme**: Let $A\in\mathbb R^{n\times m}, B$ the non-zero eigenvalues of $AB$ and $BA$ are the same

**Proof**:

$$
\underbrace{\begin{bmatrix}I&0\\B&I\end{bmatrix}}_{P}\underbrace{\begin{bmatrix}I&0\\-B&I\end{bmatrix}}_{P^{-1}}=\begin{bmatrix}I&0\\0&I\end{bmatrix}=I
$$

$$
\underbrace{\begin{bmatrix}I&0\\B&I\end{bmatrix}}_{P}\begin{bmatrix}AB&A\\0&0\end{bmatrix}\underbrace{\begin{bmatrix}I&0\\-B&I\end{bmatrix}}_{P^{-1}}=\begin{bmatrix}0&A\\0&BA\end{bmatrix}
$$

**Corollary**:

$$
\text{Tr}(AB)=\text{Tr}(BA),\quad \text{Tr}(ABC)=\text{Tr}(BCA)=\text{Tr}(CAB)
$$

$$
\text{Tr}(ABC)\neq\text{Tr}(ACB)
$$

### Cholesky Decomposition

If $A\succeq0$, then $\exists$ a lower triangular matrix $L$ with **real** and **non-negative diagonal entrie**s such that

$$
A=LL^T=\begin{bmatrix}
\ddots & & 0 \\
& \ddots & \\
\ddots & & \ddots
\end{bmatrix}\begin{bmatrix}
\ddots & & \ddots \\
& \ddots & \\
0 & & \ddots
\end{bmatrix}
$$

$$
A=\begin{bmatrix}a_{11}&\mathbf a_{12}\\\mathbf a_{21}&A_{22}\end{bmatrix},\quad L=\begin{bmatrix}l_{11}&\mathbf 0\\\mathbf l_{21}&L_{22}\end{bmatrix}
$$

$$
\begin{bmatrix}a_{11}&\mathbf a_{12}\\\mathbf a_{21}&A_{22}\end{bmatrix}=\begin{bmatrix}l_{11}&\mathbf 0\\\mathbf l_{21}&L_{22}\end{bmatrix}\begin{bmatrix}l_{11}&\mathbf l_{21}^T\\\mathbf 0&L_{22}^T\end{bmatrix}=\begin{bmatrix}l_{11}^2 & l_{11}\mathbf l_{21}^T\\l_{11}\mathbf l_{21}&\mathbf l_{21}\mathbf l_{21}^T+L_{22} L_{22}^T\end{bmatrix}
$$

$$
l_{11}=\sqrt{a_{11}},\quad\mathbf l_{21}=\frac1{l_{11}}\mathbf a_{21},\quad L_{22}L_{22}^T=A_{22}-\mathbf l_{21}\mathbf l_{21}^T
$$

**Recursive Calculation !!!**

### Positive Definite

Already very familiar, omit

### Matrix Inversion Lemma

For matrix $A$ and $B$

$$
(A+B)^{-1} = A^{-1}(I+BA^{-1})^{-1}
$$

For any matrix $A,B,C,D$ with compatible dimensions, $A,C$ nonsingular, then

$$
\begin{aligned}
(A+BCD)^{-1} &= [A(I+A^{-1}BCD)]^{-1} \\
&= (I+A^{-1}BCD)^{-1}(I+A^{-1}BCD-A^{-1}BCD)A^{-1} \\
&= [I-(I+A^{-1}BCD)^{-1}A^{-1}BCD]A^{-1} \\
&= A^{-1}-(I+A^{-1}BCD)^{-1}A^{-1}BCDA^{-1} \\
&= A^{-1}-(I+A^{-1}BCDA^{-1}A)^{-1}A^{-1}BCDA^{-1} \\
&= A^{-1}-A^{-1}BCDA^{-1}(I+AA^{-1}BCDA^{-1})^{-1} \\
&= A^{-1}-A^{-1}B[CDA^{-1}(I+BCDA^{-1})^{-1}] \\
&= A^{-1}-A^{-1}B[(I+CDA^{-1}B)^{-1}CDA^{-1}] \\
&= A^{-1}-A^{-1}B[CC^{-1}+CDA^{-1}B]^{-1}CDA^{-1} \\
&= A^{-1}-A^{-1}B[C(C^{-1}+DA^{-1}B)]^{-1}CDA^{-1} \\
&= A^{-1}-A^{-1}B(C+DA^{-1}B)^{-1}DA^{-1} \\
\end{aligned}
$$

#### Schur Complement

$$
\begin{bmatrix}
\rm A & \rm B \\ \rm C & \rm D
\end{bmatrix}=\begin{bmatrix}
\rm I & 0 \\ \rm CA^{-1} & \rm I
\end{bmatrix}\begin{bmatrix}
\rm A & 0 \\ 0 & \rm D-CA^{-1}B
\end{bmatrix}\begin{bmatrix}
\rm I & \rm A^{-1}B \\ 0 & \rm I
\end{bmatrix}
$$

$$
\begin{bmatrix}
\rm A & \rm B \\ \rm C & \rm D
\end{bmatrix}=\begin{bmatrix}
\rm I & \rm BD^{-1} \\ 0 & \rm I
\end{bmatrix}\begin{bmatrix}
\rm A-BD^{-1}C & 0 \\ 0 & \rm D
\end{bmatrix}\begin{bmatrix}
\rm I & 0 \\ \rm D^{-1}C & \rm I
\end{bmatrix}
$$

### Inner Product Space

$\mathbf u,\mathbf v\in\mathcal V$, the inner product $\langle\mathbf u,\mathbf v\rangle$ satisfies

1. **Linearity**: $\langle\alpha_1\mathbf u_1+\alpha_2\mathbf u_2,\mathbf v\rangle=\alpha_1\langle\mathbf u_1,\mathbf v\rangle+\alpha_2\langle\mathbf u_2,\mathbf v\rangle$
2. **Conjugate Symmetry**: $\langle\mathbf u,\mathbf v\rangle=\langle\mathbf v,\mathbf u\rangle^*$, $(·)^*$ means transpose
3. **Positive Definiteness**: $||\mathbf u||^2=\langle\mathbf u,\mathbf u\rangle=0\Leftrightarrow\mathbf u=0$

For two random variables $X,Y$, define $\langle X,Y\rangle=E[XY^T]$

#### Projection Theorem

Let $\mathcal H\in\mathbb R^{m}$ be a linear **subspace** of $\mathcal S\in\mathbb R^n,(m<n)$. For some vector $\mathbf y\in\mathcal S$, the projection of $\mathbf y$ onto $\mathcal H$ denoted as $\hat{\mathbf y}_{\mathcal H}$ is a **uniyque** element in $\mathcal H$, such that $\forall\mathbf x\in\mathcal H,\langle\mathbf y-\hat{\mathbf y}_{\mathcal H},\mathbf x\rangle=0$, in other word $\mathbf y-\hat{\mathbf y}_{\mathcal H}\perp\mathbf x$.

### Gram-Schmidt Process

Let $\set{\mathbf v_1,\mathbf v_2,...\mathbf v_n}$ be a set of **linearly independent** vectors in an inner product space VV. The Gram-Schmidt process constructs an **orthonormal basis** $\set{\mathbf u_1,\mathbf u_2,\cdots,\mathbf u_n}$ for the subspace spanned by $\set{\mathbf v_1,\mathbf v_2,...\mathbf v_n}$ as follows:

$$
\begin{aligned}
\mathbf u_1 &= \mathbf v_1 \\
\mathbf u_2 &= \mathbf v_2 - \text{proj}_{\mathbf u_1}(\mathbf v_2) \\
&\quad\vdots \\
\mathbf u_k &= \mathbf v_k - \sum_{j=1}^{k-1}\text{proj}_{\mathbf u_j}(\mathbf v_k)
\end{aligned}\qquad\mathbf e_i = \frac{\mathbf u_i}{||\mathbf u_i||}
$$

## Controllability & Observability

### Autonomous System

A linear system $x_{k+1}=Ax_k$ is said to be stable if

$$
\forall x_0,\lim_{k\to\infty}|x_k|=0
$$

The system is stable if and only if

$$
\max_i|\lambda_i(A)|<1
$$

### Controllability

A linear system $x_{k+1}=Ax_k+Bu_k$ is said to be controllable if

$$
\forall x_0,x^*,\exists k>0,\mathbf u_k=[u_{k-1},\cdots,u_1,u_0],\quad\text{s.t.}\quad x_k=x^*.
$$

$(A,B)$ is **controllable** is equivalent to the following

1. $M_c=[B,AB,A^2B,\cdots,A^{n-1}B]$ is full rank
2. $W_c=\sum_{k=0}^{n-1}A^kBB^T(A^T)^k$ is full rank
3. **PBH test**: $\forall\lambda\in\mathbb C,[A-\lambda I, B]$ is full rank

Assume $(A,B)$ is controllable, given $x_0, x^*$, find $\mathbf u_n$ such that $x_n=x^*$

$$
\begin{aligned}
x^* = x_n &= Ax_{n-1} + Bu_{n-1} \\
&= A(Ax_{n-2} + Bu_{n-2}) + Bu_{n-1} \\
&= A^2x_{n-2} + ABu_{n-2} + Bu_{n-1} \\
&= A^nx_0 + A^{n-1}Bu_0 + \cdots + ABu_{n-2} + Bu_{n-1} \\
&= A^nx_0 + M_c\mathbf u_n
\end{aligned}
$$

$$
\mathbf u_n = M_c^T(M_cM_c^T)^{-1}(x^*-A^nx_0)
$$

### Observability

A linear system $x_{k+1}=Ax_k, y_k=Cx_k$ is said to be observable if $\forall x_0,\exists k>0$, such that $x_0$ can be computed from $\mathbf y_k=[y_0,y_1,\cdots,y_{k-1}]^T$.

$(A,C)$ is **observable** is equivalent to the following

1. $M_o=\begin{bmatrix}C\\CA\\\vdots\\CA^{n-1}\end{bmatrix}$ is full rank
2. $W_o=\sum_{k=0}^{n-1}(A^k)^TC^TCA^k$ is full rank
3. **PBH test**: $\forall\lambda\in\mathbb C,\begin{bmatrix}A-\lambda I\\C\end{bmatrix}$ is full rank

Assume $(A,C)$ is observable, find $x_0$ from $\mathbf y_k$.

$$
\begin{aligned}
y_0 &= Cx_0 \\
y_1 &= Cx_1 = CAx_0 \\
&\ \ \vdots \\
y_{n-1} &= CA^{n-1}x_0
\end{aligned}\Rightarrow \mathbf y_n=\begin{bmatrix}y_0\\y_1\\\vdots\\y_{n-1}\end{bmatrix}=\begin{bmatrix}C\\CA\\\vdots\\CA^{n-1}\end{bmatrix}x_0=M_ox_0
$$

$$
x_0=(M_o^TM_o)^{-1}M_o^T\mathbf y_n
$$

### Duality

$(A,C)$ is observable if and only if $(A^T,C^T)$ is controllable.

## Estimation Problem

### Maximum A Posterior Estimation (MAP)

$x$ is the parameter to be estimated

$$
\hat x=\arg\max_x \begin{cases}
f(x|y),&x\text{ is continuous} \\
p(x|y),&x\text{ is discrete} \\
\end{cases}
$$

### Minimum Mean Squared Error Estimation (MMSE)

$$
\hat x=\arg\min_{\hat x}E[e^Te|y]=\arg\min_{\hat x}E[\hat x|y],\ e=x-\hat x
$$

$$
\hat x=\int x\cdot f(x|y)\ {\rm d}x\quad\text{or}\quad\sum x\cdot p(x|y)
$$

**Proof**:

$$
\begin{aligned}
E[e^Te] &= E[(x-\hat x)^T(x-\hat x)|y] \\
&= E[x^Tx|y] - 2\hat x^TE[x|y]+\hat x^T\hat x \\
\end{aligned}
$$

$$
\frac\partial{\partial\hat x}(E[x^Tx|y] - 2\hat x^TE[x|y]+\hat x^T\hat x)=0
$$

$$
-2E[X|Y]+2\hat x=0
$$

$$
\hat x_\text{MMSE} = E[X|Y]
$$

### Maximum Likelihood Estimation (ML)

Non Bayesian. $p(y|x)$ is conditional probability and $p(y;x)$ is parameterized probability, $p(y|x)\not\Leftrightarrow p(y;x)$.

Assume we have $n$ measurements $\mathcal X=(X_1,\cdots,X_n)$, we use $p(\mathcal X;\theta)$ to describe the joint probability of $\mathcal X$.

$$
\hat\theta_n=\arg\max_\theta\begin{cases}
f(\mathcal X;\theta),&\theta\text{ is continuous} \\
p(\mathcal X;\theta),&\theta\text{ is discrete} \\
\end{cases}
$$

$$
p(\mathcal X;\theta)=\prod_{i=1}^np(X_i;\theta)\quad\Leftrightarrow\quad\log p(\mathcal X;\theta)=\sum_{i=1}^n\log p(X_i;\theta)
$$

**MAP & ML**:

$$
\begin{aligned}
\hat\theta_\text{MAP} &= \arg\max_{\theta} p(\theta|x)=\arg\max_\theta\frac{p(\theta)p(x|\theta)}{p(x)}p(\theta|x)=\arg\max_\theta p(\theta)p(x|\theta) \\
\hat\theta_\text{ML} &= \arg\max_\theta p(x;\theta)
\end{aligned}
$$

### Weighted Least Square Estimation

$$
\mathcal E(x) = ||Ax-b||^2_\Sigma=x^TA^T\Sigma^{-1}Ax-2b^T\Sigma^{-1}Ax+b^T\Sigma^{-1}b
$$

$$
\nabla\mathcal E = 2A^T\Sigma^{-1}Ax - 2A^T\Sigma^{-1}b
$$

$$
\hat x = (A^T\Sigma^{-1}A)^{-1}A^T\Sigma^{-1}b
$$

### Linear Minimum Mean Square Error Estimation (LMMSE)

LMMSE estimation wants to find a linear estimator

$$
\hat x=Ky+b
$$

such that minimize the mean square error

$$
\begin{aligned}
\text{MSE} &= E[(x-\hat x)^T(x-\hat x)] \\
&= E[x^Tx]-2E[x^T\hat x]+E[\hat x^T\hat x] \\
&= E[x^Tx]-2E[x^T(Ky+b)]+E[(Ky+b)^T(Ky+b)] \\
\end{aligned}
$$

$$
\frac{\partial\text{MSE}}{\partial b} = -2E[x]+2b+2KE[y] = 0
$$

$$
b=\mu_x-K\mu_y
$$

$$
\begin{aligned}
\text{MSE} &= E[x^Tx]-2E[x^T(Ky+b)]+E[(Ky+b)^T(Ky+b)] \\
&= E[x^Tx]-2E[x^T(Ky+\mu_x-K\mu_y)]+E[(Ky+\mu_x-K\mu_y)^T(Ky+\mu_x-K\mu_y)] \\
&= E[x^Tx]-2E[x^TKy]-2\mu_x^T\mu_x+2\mu_x^TK\mu_y+E[y^TK^TKy]+\mu_x^T\mu_x-2\mu_x^TK\mu_y+\mu_y^TK^TK\mu_y
\end{aligned}
$$

$$
\frac{\partial\text{MSE}}{\partial K} = -2\Sigma_{xy}+2K\Sigma_{yy}=0
$$

$$
K=\Sigma_{xy}\Sigma_{yy}^{-1}
$$

$$
\hat x=Ky+b=\mu_x+\Sigma_{xy}\Sigma_{yy}^{-1}(y-\mu_y)
$$

$$
\Sigma_{\hat x\hat x} = \Sigma_{xx}-\Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx}
$$

#### Orthogonality Principle

$$
\begin{aligned}
\langle x-Ky-b,y\rangle &= E[(x-Ky-b)y^T] \\
&= E[xy^T]-KE[yy^T]-bE[y^T] \\
&= \Sigma_{xy}+\mu_x\mu_y^T-K(\Sigma_{yy}+\mu_y\mu_y^T)-(\mu_x-K\mu_y)\mu_y^T \\
&= \Sigma_{xy} - (\Sigma_{xy}\Sigma_{yy}^{-1})\Sigma_{yy} \\
&=0
\end{aligned}
$$

$$
x-(Ky+b)\perp y
$$

This shows that error $e=x-\hat x$ is independent of observation $y$.

#### Innovation Process

Calculating $\Sigma_{yy}$ consumes lots of time, however, if $\Sigma_{yy}$ is diagonal the thing becomes easy. By G.S. process, we can obtain orthogonality vectors $\vec e_1,\cdots\vec e_k$ and the lower triangular transform matrix $F$ from $\vec y_1,\cdots,\vec y_k$. The key idea of ​​orthogonal projection is to decompose the observation vector $y_k$ into **a part related to the past prediction value**, which can be predicted by $y_1,\cdots y_{k-1}$, and **a new part that is irrelevant to the past prediction value (innovation)**.

$$
e=Fy
$$

Then the covariance can be calculated by

$$
\Sigma_{ee}=F\Sigma_{yy}F^T,\quad\Sigma_{ex}=F\Sigma_{yx}
$$

$$
K_e=\Sigma_{ex}\Sigma_{ee}^{-1}=F\Sigma_{yx}(F^T)^{-1}\Sigma_{yy}^{-1}F^{-1}
$$

Although $K_e$ is not equal to $K$, it serves as the Kalman gain in the transformed or projected space defined by the matrix $F$.

For new coming $\vec y_{t+1}$ we can find $\vec e^{k+1}$ by G.S. process

$$
\begin{aligned}
e_{k+1}&=y_{k+1}-\hat y_{k+1|k}\\
&=y_{k+1}-\text{proj}(y_{k+1};\mathcal E_k)\\
&=y_{k+1}-\sum_{i=1}^k\frac{\langle y_{k+1},e_i\rangle}{\langle e_i,e_i\rangle}\cdot e_i
\end{aligned}
$$

It satisfies

$$
\langle e_{k+1},y_{i}\rangle=E[e_{k+1}y_{i}^T]=0,\quad\forall i\in[1,k]
$$

To estimate $x$ at $k+1$

$$
\begin{aligned}
\hat x_{k+1}&=\text{proj}(x_{k+1};\mathcal E_{k+1})\\
&=\sum_{i=1}^{k+1}\frac{\langle x_{k+1},e_i\rangle}{\langle e_i,e_i\rangle}\cdot e_i \\
&=\sum_{i=1}^k\frac{\langle x_{k+1},e_i\rangle}{\langle e_i,e_i\rangle}\cdot e_ + \frac{\langle x_{k+1},e_{k+1}\rangle}{\langle e_{k+1},e_{k+1}\rangle}\cdot e_{k+1}\\
&=\sum_{i=1}^k\frac{\langle x_k,e_i\rangle}{\langle e_i,e_i\rangle}\cdot e_i + \frac{\langle x_{k+1},e_{k+1}\rangle}{\langle e_{k+1},e_{k+1}\rangle}\cdot e_{k+1}\\
&= \hat x_k+ \frac{\langle x_{k+1},e_{k+1}\rangle}{\langle e_{k+1},e_{k+1}\rangle}\cdot e_{k+1}
\end{aligned}
$$

<!-- #TODO: Prove $\langle x_{k+1},e_i\rangle=\langle x_k,e_i\rangle,\forall i\in[1,k]$ -->

## Kalman Filter

### Takeaway Notes

Consider an LTI system with initial conditions $\hat x_{0\vert0}$ and $\hat P_{0\vert0}$

$$
\begin{aligned}
x_{k+1} &= Ax_k+Bu_k+\omega_k,&\omega_k\sim\mathcal N(0,Q) \\
y_k &= Cx_k + \nu_k, &\nu_k\sim\mathcal N(0,R)
\end{aligned}
$$

Find the estimation of $x_k$ given $\set{u_0,u_1\cdots,u_k}$ and $\set{y_0,y_1\cdots,y_k}$.

Assumptions:

1. $(A,B)$ is controllable and $(A,C)$ is observable
2. $Q\succeq0,R\succeq0,P_0\succeq0$
3. $\omega_k$, $\nu_k$ and $\hat x_0$ are mutually uncorelated
4. The future state of the system is conditionally independent of the past states given the current state

Time Update:

$$
\begin{aligned}
\hat x_{k\vert k-1}&=A\hat x_{k-1\vert k-1}+Bu_k \\
\hat P_{k\vert k-1} &= A\hat P_{k-1\vert k-1}A^T+Q
\end{aligned}
$$

Measurement Update:

$$
\begin{aligned}
K_k &= \hat P_{k\vert k-1}C^T(C\hat P_{k\vert k-1}C^T+R)^{-1} \\
\hat x_{k\vert k} &= \hat x_{k\vert k-1} + K_k(y_k-C\hat x_{k\vert k-1}) \\
\hat P_{k\vert k} &= \hat P_{k\vert k-1}-K_kC\hat P_{k\vert k-1} = (\hat P_{k\vert k-1}^{-1}+C^TR^{-1}C)^{-1}\\
\end{aligned}
$$

### Linear Minimum Mean Square Estimation

### Bayesian Posterior Estimation


$$
\begin{aligned}
&p(x_{k}|y_{1:k}, u_{1:k})\\=&p(x_{k}|y_k,y_{1:k-1}, u_{1:k})\\=&\frac{p(y_{k}|x_k,y_{1:k-1}, u_{1:k})\cdot p(x_{k}|y_{1:k-1}, u_{1:k})}{p(y_{k}|y_{1:k-1}, u_{1:k})}\\
=&\eta\cdot\underbrace{p(y_k|x_k)}_\text{observation model}\cdot\underbrace{p(x_{k}|y_{1:k-1}, u_{1:k})}_\text{prior belief} \\
=&\eta\cdot p(y_k|x_k)\cdot\int p(x_{k},x_{k-1}|y_{1:k-1}, u_{1:k})\ {\rm d}x_{k-1} \\
=&\eta\cdot p(y_k|x_k)\cdot\int p(x_{k}|x_{k-1},y_{1:k-1},u_{1:k})\cdot p(x_{k-1}|y_{1:k-1},u_{1:k})\ {\rm d}x_{k-1} \\
=&\eta\cdot p(y_k|x_k)\cdot\int\underbrace{p(x_{k}|x_{k-1},u_{k})}_\text{motion model}\cdot\underbrace{p(x_{k-1}|y_{1:k-1},u_{1:k-1})}_\text{previous belief}\ {\rm d}x_{k-1} \\
\end{aligned}
$$
Applying Bayesian Rule and Markov Assumptions to $p(x_k|y_{1:k},u_{1:k})$, then the time update and the measurement update becomes very explicit.



## Least Square Regulator
